# ==========================================================
# predict_accounts_v3_dual.py
# åŠŸèƒ½ï¼š
#   âœ… è‡ªå‹•è¼‰å…¥æœ€ä½³é–¾å€¼ best_threshold.json
#   âœ… è‡ªå‹•è£œç¼ºå¸³è™Ÿç‰¹å¾µï¼ˆä»¥å¹³å‡å€¼å¡«è£œï¼‰
#   âœ… è‡ªå‹•åµæ¸¬ GPU / CPU
#   âœ… ç¢ºä¿ acct å°é½Šä¸æœƒå…¨ç‚º 0
# ==========================================================

import pandas as pd
import numpy as np
import joblib
import os
import json

# ==========================
# 1ï¸âƒ£ è·¯å¾‘è¨­å®š
# ==========================
feature_file = "feature_data_v3/account_features.csv"
predict_file = "dataset/acct_predict.csv"
model_file = "train_data_v3/lightgbm_model.pkl"
threshold_file = "train_data_v3/best_threshold.json"
output_dir = "results_v3"
os.makedirs(output_dir, exist_ok=True)

output_full = os.path.join(output_dir, "predict_full_final.csv")
submit_file = os.path.join(output_dir, "predict_for_submit_final.csv")

# ==========================
# 2ï¸âƒ£ è¼‰å…¥è³‡æ–™èˆ‡æ¨¡å‹
# ==========================
print(f"ğŸ“‚ è¼‰å…¥ç‰¹å¾µè¡¨ï¼š{feature_file}")
features = pd.read_csv(feature_file)

print(f"ğŸ“‚ è¼‰å…¥é æ¸¬å¸³æˆ¶ï¼š{predict_file}")
predict_accts = pd.read_csv(predict_file)

print(f"ğŸ“¦ è¼‰å…¥ LightGBM æ¨¡å‹ï¼š{model_file}")
model = joblib.load(model_file)

# ==========================
# 3ï¸âƒ£ çµ±ä¸€å¸³è™Ÿæ¬„ä½åç¨±
# ==========================
acct_col = None
for col in predict_accts.columns:
    if col.strip().lower() in ["acct", "acct_id"]:
        acct_col = col
        break
if acct_col is None:
    raise ValueError("âŒ acct_predict.csv ä¸­æ²’æœ‰æ‰¾åˆ°å¸³è™Ÿæ¬„ä½ (acct æˆ– acct_id)")

predict_accts.rename(columns={acct_col: "acct_id"}, inplace=True)
features["acct_id"] = features["acct_id"].astype(str).str.strip()
predict_accts["acct_id"] = predict_accts["acct_id"].astype(str).str.strip()

# ==========================
# 4ï¸âƒ£ è½‰æ›æ™‚é–“æ¬„ä½
# ==========================
for col in ["first_txn", "last_txn"]:
    if col in features.columns:
        features[col] = pd.to_datetime(features[col], errors="coerce")
        features[col] = (features[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta("1s")
        print(f"ğŸ•’ å·²å°‡æ¬„ä½ {col} è½‰æ›ç‚º UNIX ç§’æ•¸æ ¼å¼")

# ==========================
# 5ï¸âƒ£ å°é½Šç‰¹å¾µæ¬„ä½
# ==========================
train_features = model.feature_name()
extra_cols = [c for c in features.columns if c not in train_features]
if extra_cols:
    print(f"âš ï¸ å¤šå‡ºæ¬„ä½: {extra_cols}")

# è‡ªå‹•è£œç¼ºå¤±æ¬„ä½
for c in train_features:
    if c not in features.columns:
        features[c] = 0

# ==========================
# 6ï¸âƒ£ æ¯”å°å¸³è™Ÿè¦†è“‹ç‡
# ==========================
matched = features["acct_id"].isin(predict_accts["acct_id"])
unmatched_count = len(predict_accts) - matched.sum()
if unmatched_count > 0:
    print(f"âš ï¸ æœ‰ {unmatched_count} ç­†å¸³è™Ÿæ²’æœ‰ç‰¹å¾µï¼Œå°‡è‡ªå‹•è£œå¹³å‡å€¼ç‰¹å¾µ")

# è¤‡è£½å¹³å‡ç‰¹å¾µä½œç‚ºè£œå€¼æ¨£æ¿
mean_row = features[train_features].mean()
missing_accts = predict_accts.loc[~predict_accts["acct_id"].isin(features["acct_id"]), "acct_id"]
missing_features = pd.DataFrame([mean_row] * len(missing_accts))
missing_features.insert(0, "acct_id", missing_accts.values)

# åˆä½µè£œè¶³å¾Œå®Œæ•´ç‰¹å¾µè¡¨
features_fixed = pd.concat([features, missing_features], ignore_index=True)

# é¸å‡ºé æ¸¬åå–®
predict_features = features_fixed[features_fixed["acct_id"].isin(predict_accts["acct_id"])].copy()
X_pred = predict_features.reindex(columns=train_features, fill_value=0)

print(f"ğŸ”¹ ä½¿ç”¨ç‰¹å¾µæ•¸é‡: {X_pred.shape[1]}ï¼Œé æ¸¬ç­†æ•¸: {len(X_pred)}")

# ==========================
# 7ï¸âƒ£ è¼‰å…¥æœ€ä½³é–¾å€¼
# ==========================
threshold = 0.5
if os.path.exists(threshold_file):
    with open(threshold_file, "r") as f:
        data = json.load(f)
        threshold = data.get("best_threshold", 0.5)
    print(f"ğŸ§  è‡ªå‹•è¼‰å…¥è¨“ç·´æœ€ä½³é–¾å€¼: {threshold:.3f}")
else:
    print(f"âš ï¸ æœªæ‰¾åˆ° {threshold_file}ï¼Œä½¿ç”¨é è¨­ 0.5")

# ==========================
# 8ï¸âƒ£ é æ¸¬éšæ®µ
# ==========================
device_info = "GPU" if "gpu" in str(model.params.get("device", "")).lower() else "CPU"
print(f"ğŸš€ ä½¿ç”¨ {device_info} é€²è¡Œé æ¸¬ä¸­ ...")

y_pred_prob = model.predict(X_pred, num_iteration=getattr(model, "best_iteration", None))
y_pred_prob = np.array(y_pred_prob).flatten()

predict_features["probability"] = y_pred_prob
predict_features["label"] = (y_pred_prob >= threshold).astype(int)

print(f"âœ… å·²å®Œæˆæ¨¡å‹é æ¸¬ï¼Œå…± {len(predict_features)} ç­†ã€‚")

# ==========================
# ğŸ”§ åˆä½µé˜²å‘†ï¼šçµ±ä¸€å¸³è™Ÿæ ¼å¼
# ==========================
predict_accts["acct_id"] = (
    predict_accts["acct_id"].astype(str).str.strip().str.lower()
)
predict_features["acct_id"] = (
    predict_features["acct_id"].astype(str).str.strip().str.lower()
)

# æ¯”å°äº¤é›†èˆ‡å·®é›†
common = set(predict_accts["acct_id"]) & set(predict_features["acct_id"])
missing = set(predict_accts["acct_id"]) - common
print(f"ğŸ“Š åŒ¹é…å¸³è™Ÿæ•¸ï¼š{len(common)} / {len(predict_accts)}")
if len(missing) > 0:
    print(f"âš ï¸ æœ‰ {len(missing)} ç­†å¸³è™Ÿä»ç„¡æ³•å°ä¸Šç‰¹å¾µï¼ˆå°‡è‡ªå‹•è£œ0ï¼‰")
    print("ğŸ” å‰5ç­†æœªåŒ¹é…å¸³è™Ÿï¼š", list(missing)[:5])

# ==========================
# ğŸ”— åˆä½µå› acct_predict åå–® ...
# ==========================
print("ğŸ”— åˆä½µå› acct_predict åå–® ...")

submission = pd.merge(
    predict_accts,
    predict_features[["acct_id", "label", "probability"]],
    on="acct_id",
    how="left"
)

# ğŸ§© ä¿®æ­£é‡è¤‡æ¬„ä½ï¼ˆlabel_x, label_yï¼‰
if "label_x" in submission.columns and "label_y" in submission.columns:
    print("âš™ï¸ åµæ¸¬åˆ° label_x / label_yï¼Œé€²è¡Œåˆä½µ ...")
    submission["label"] = submission["label_y"].combine_first(submission["label_x"])
    submission.drop(columns=["label_x", "label_y"], inplace=True)

# ğŸ§© è‹¥ä»ç„¡ label æ¬„ä½ â†’ è£œä¸Š 0
if "label" not in submission.columns:
    print("âŒ è­¦å‘Šï¼šåˆä½µå¾Œå®Œå…¨æ²’æœ‰ label æ¬„ä½ï¼Œå°‡è£œä¸Šå…¨ 0ã€‚")
    submission["label"] = 0

# å¡«è£œç¼ºå€¼
submission["label"] = submission["label"].fillna(0).astype(int)
submission["probability"] = submission["probability"].fillna(0)

# ==========================
# ğŸ”Ÿ è¼¸å‡ºçµæœ
# ==========================
submission.rename(columns={"acct_id": "acct"}, inplace=True)
submission["acct"] = submission["acct"].astype(str)
submission["label"] = submission["label"].astype(int)

submission.to_csv(output_full, index=False)
submission[["acct", "label"]].to_csv(submit_file, index=False)

# ==========================
# ğŸ“Š çµæœæ‘˜è¦
# ==========================
print(f"ğŸ’¾ å·²è¼¸å‡ºå®Œæ•´çµæœï¼š{output_full}")
print(f"ğŸ å·²è¼¸å‡ºæ¯”è³½ç”¨çµæœï¼ˆacct,label æ ¼å¼ï¼‰ï¼š{submit_file}")
print("âœ… label å”¯ä¸€å€¼ =", submission["label"].unique())
print("âœ… acct æ•¸é‡ =", len(submission))
print("ğŸ“Š label åˆ†å¸ƒï¼š")
print(submission["label"].value_counts(normalize=True))

print("âœ… å®Œæˆï¼Œå¯ç›´æ¥ä¸Šå‚³ AI CUPï¼")
